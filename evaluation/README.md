# Evaluation Matrix
- randomly sampled 20 instructional cooking tutorial videos from the Cook2 dataset2. 

## User-Initiated Responses
- For each video, generate ?? different **queries** for each of the 6 user-initiated events.

### User-Initiated Events
```js
{
    "step_related": `User asks about step related questions
        - Questions about current, previous, or future steps in the cooking process
        - Examples:
            * "What's the next step I should do?"
            * "What steps did I miss?"
            * "What step am I on right now?"
            * "What should I do next?" 
            * "Was I supposed to preheat the oven?"`,

    "problem_fixing": `User perceives a problem and asks how to fix something
        - Requests for correction or problem-solving
        - Examples:
            * "The sauce is too thick, how do I fix it?"
            * "I added too much salt, what should I do?"
            * "The dough isn't rising, how can I fix this?"
            * "I burned the steak, can this be saved?"`,

    "repeat_interaction": `User asks for repeating a previous interaction
        - User seeks to recall the agent's response from the previous interaction.
        - Examples:
            * "Can you say that again?"
            * "I didn't catch you."
            * "What did you say about XXXX?"
            * "Please repeat the last instruction"
            * "How did you say about the ingredients for making the sauce?"`,

    "general_question": `User asks for general questions
        - General cooking queries based on the video knowledge except for a specific steps or food states
        - Examples:
            * "What other ingredients do we need?"
            * "How many steps are still left?"`,

    "visual_confirmation": `User asks about confirmations of visual elements in the cooking scene. 
        - Seeking visual description and verification of food states
        - Examples:
            * "Can you explain the current scene for me?"
            * "What is the current state of the food?"
            * "What are things around me now?"
            * "I'm a chopping the onion in a right way?"
            * "Is this the right ingredient?"
            * "How does my steak look like now?"`,

    "previous_step_recall": `User seeks to retrieve previous steps or interactions
    - Asking for recall of previous steps or actions
    - Examples:
        * "What's my last step?"
        * "What did I do before this?"
        * "What did I add last?"
        * "What are my last three steps?"`,
}
```

### User Stream Annotation
- format example:
```JSON
{
    "video_id": "P3",
    "ingredients": [
        "1 potato",
        "Soy sauce",
        "Some flakes of Chili peppers",
        "Cooking Oil"
    ],
    "annotations": [
        {
            "segment": [
                40,
                67
            ],
            "id": 0,
            "sentence": "Wash a potato."
        },
        {
            "segment": [
                121,
                300
            ],
            "id": 1,
            "sentence": "Peel the potato."
        },
        {
            "segment": [
                376,
                815
            ],
            "id": 2,
            "sentence": "Cut the potato into slices and then shred it. Store in a bowl."
        },
        {
            "segment": [
                913,
                940
            ],
            "id": 3,
            "sentence": "Prepare and wash the chili peppers."
        },
        {
            "segment": [
                960,
                1037
            ],
            "id": 4,
            "sentence": "Hand-pick and remove the bottom part of the chili peppers."
        },
        {
            "segment": [
                1075,
                1132
            ],
            "id": 5,
            "sentence": "Cut the chili peppers into pieces and store them in a pan."
        },
        {
            "segment": [
                1234,
                1239
            ],
            "id": 6,
            "sentence": "Add some oil to the pan."
        },
        {
            "segment": [
                1415,
                1487
            ],
            "id": 7,
            "sentence": "Turn on the heat, add the shredded potatoes to the pan, then stir."
        },
        {
            "segment": [
                1489,
                1500
            ],
            "id": 8,
            "sentence": "Add soy sauce to the pan."
        },
        {
            "segment": [
                1502,
                1631
            ],
            "id": 9,
            "sentence": "Stir the shredded potato for 2 minutes, then serve."
        }
    ],
    "notes": [
        "Prefer use one type of sauce fits all."
    ]
}
```

- Each query includes: 
  - a natural language query (generated by a LLM) 
  - a screenshot of cooking extracted from other tutorial videos
  - history

- **prompt** for generating natural language queries:
```
Suppose you are a visually impaired user who is cooking. An agent can analyze your cooking condition in real-time. During cooking, you can ask six types of questions to the agent:

<User-Initiated Questions>
{
    "step_related": `User asks about step related questions
        - Questions about current, previous, or future steps in the cooking process
        - Examples:
            * "What's the next step I should do?"
            * "What steps did I miss?"
            * "What step am I on right now?"
            * "What should I do next?" 
            * "Was I supposed to preheat the oven?"`,

    "problem_fixing": `User perceives a problem and asks how to fix something
        - Requests for correction or problem-solving
        - Examples:
            * "The sauce is too thick, how do I fix it?"
            * "I added too much salt, what should I do?"
            * "The dough isn't rising, how can I fix this?"
            * "I burned the steak, can this be saved?"`,

    "repeat_interaction": `User asks for repeating a previous interaction
        - User seeks to recall the agent's response from the previous interaction.
        - Examples:
            * "Can you say that again?"
            * "I didn't catch you."
            * "What did you say about XXXX?"
            * "Please repeat the last instruction"
            * "How did you say about the ingredients for making the sauce?"`,

    "general_question": `User asks for general questions
        - General cooking queries based on the video knowledge except for a specific steps or food states
        - Examples:
            * "What other ingredients do we need?"
            * "How many steps are still left?"`,

    "visual_confirmation": `User asks about confirmations of visual elements in the cooking scene. 
        - Seeking visual description and verification of food states
        - Examples:
            * "Can you explain the current scene for me?"
            * "What is the current state of the food?"
            * "What are things around me now?"
            * "I'm a chopping the onion in a right way?"
            * "Is this the right ingredient?"
            * "How does my steak look like now?"`,

    "previous_step_recall": `User seeks to retrieve previous steps or interactions
        - Asking for recall of previous steps or actions
        - Examples:
            * "What's my last step?"
            * "What did I do before this?"
            * "What did I add last?"
            * "What are my last three steps?"`,
}

Your cooking process is summarized by <User Stream Annotation> (attached below). Please generate a query that you would ask the agent. 

<User Stream Annotation>

Please generate three queries for each of the six user-initiated events in the following format:
video_id: {
    "event_type": text,
    "query": text,
    "segment": [start, end]
}
Important: please ensure that in each query, the cooking process that timestamp matches is related to the query. Also, please ensure that the query is resonable (i.e., it is a question that a visually impaired user would likely to ask during cooking). Finally, only return the json response.
```

### accuracy
- a. if the system maps the user query to an incorrect event; 
- b. given a correct event mapping, if the system provides a correct response.

### quality
- evaluators rated the quality of the systemâ€™s prompt on a 7-point Likert scale, with the lowest quality being 1 and the highest quality being 7.
- aspects: length, tone, and details.

## Agent-Initiated Responses
- TODO