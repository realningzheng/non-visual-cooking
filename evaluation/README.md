# Evaluation Matrix
- randomly sampled 20 instructional cooking tutorial videos from the Cook2 dataset2. 

## User-Initiated Responses
- For each video, generate ?? different **queries** for each of the 6 user-initiated events.

### User-Initiated Events
```js
1: `User asks about step related questions
    - Questions about current, previous, or future steps in the cooking process
    - Examples:
        * "What's the next step I should do?"
        * "What steps did I miss?"
        * "What step am I on right now?"
        * "What should I do next?" 
        * "Was I supposed to preheat the oven?"`,

2: `User perceives a problem and asks how to fix something
    - Requests for correction or problem-solving
    - Examples:
        * "The sauce is too thick, how do I fix it?"
        * "I added too much salt, what should I do?"
        * "The dough isn't rising, how can I fix this?"
        * "I burned the steak, can this be saved?"`,

3: `User asks for repeating a previous interaction
    - User seeks to recall the agent's response from the previous interaction.
    - Examples:
        * "Can you say that again?"
        * "I didn't catch you."
        * "What did you say about XXXX?"
        * "Please repeat the last instruction"
        * "How did you say about the ingredients for making the sauce?"`,

4: `User asks for general questions
    - General cooking queries based on the video knowledge except for a specific steps or food states
    - Examples:
        * "What other ingredients do we need?"
        * "How many steps are still left?"`,

5: `User asks about confirmations of visual elements in the cooking scene. 
    - Seeking visual description and verification of food states
    - Examples:
        * "Can you explain the current scene for me?"
        * "What is the current state of the food?"
        * "What are things around me now?"
        * "I'm a chopping the onion in a right way?"
        * "Is this the right ingredient?"
        * "How does my steak look like now?"`,

6: `User seeks to retrieve previous steps or interactions
   - Asking for recall of previous steps or actions
   - Examples:
     * "What's my last step?"
   	 * "What did I do before this?"
	 * "What did I add last?"
	 * "What are my last three steps?"`,
```
### User Stream Annotation
- format example:
```
Ingredients:
1 round eggplant
1 green pepper
Several cloves of garlic
Cooking oil
A few pieces of Sichuan peppercorn
Soy sauce
Oyster sauce
2 teaspoon of salt
1/2 teaspoon of chicken powder

01:10 - 02:16 - Wash the eggplant and green pepper, take the green pepper seeds out.
03:00 - 07:07 - Cut the eggplant and green pepper into pieces.
07:44 - 09:55 - Peel and mince the garlic.
11:00 - 12:27 - Turn on the heat of the gas range, and add some oil to the wok. 
12:30 - 13:04 - Add Sichuan peppercorn, minced garlic to the wok, mix and wait for a few seconds.
13:10 -  14:23 - Add eggplant, soy sauce, oyster sauce and stir for 60 seconds.
14:28 - 14:42 - Add green pepper, stir for 15 second.
14:48 - 15:25 - Add 1 table spoon of water, and stir for another 40 seconds.
15:26 - 16:40 - Add 2 teaspoon of salt, add 1/2 teaspoon of chicken powder, and stir for 60 seconds.

JSON:
[
    {
        "segment": [70, 136],
        "id": 0,
        "sentence": "Wash the eggplant and green pepper, then remove the seeds from the green pepper."
    },
    {
        "segment": [180, 427],
        "id": 1,
        "sentence": "Cut the eggplant and green pepper into pieces."
    },
    {
        "segment": [464, 595],
        "id": 2,
        "sentence": "Peel and mince the garlic."
    },
    {
        "segment": [660, 747],
        "id": 3,
        "sentence": "Turn on the heat of the gas range and add some oil to the wok."
    },
    {
        "segment": [750, 784],
        "id": 4,
        "sentence": "Add Sichuan peppercorn and minced garlic to the wok, mix, and wait for a few seconds."
    },
    {
        "segment": [790, 863],
        "id": 5,
        "sentence": "Add the eggplant, soy sauce, and oyster sauce, and stir for 60 seconds."
    },
    {
        "segment": [868, 882],
        "id": 6,
        "sentence": "Add the green pepper and stir for 15 seconds."
    },
    {
        "segment": [888, 925],
        "id": 7,
        "sentence": "Add 1 tablespoon of water and stir for another 40 seconds."
    },
    {
        "segment": [926, 1000],
        "id": 8,
        "sentence": "Add 2 teaspoons of salt and 1/2 teaspoon of chicken powder, then stir for 60 seconds."
    }
]
```

- Each query includes: 
  - a natural language query (generated by a LLM) 
  - a screenshot of cooking extracted from other tutorial videos
  - history

- **prompt** for generating natural language queries:
```
Suppose you are a visually impaired user who is cooking. An agent can analyze your cooking condition in real-time. During cooking, you can ask six types of questions to the agent:

<User-Initiated Questions>

Your cooking process is summarized by <User Stream Annotation> (attached below). Please generate a query that you would ask the agent. 

<User Stream Annotation>

Please generate three queries for each of the six user-initiated events in the following format:
{
    "event_type": text,
    "query": text,
    "timestamp": text
}
Important: please ensure that in each query, the cooking process that timestamp matches is related to the query. Also, please ensure that the query is resonable (i.e., it is a question that a visually impaired user would likely to ask during cooking).
```

### accuracy
- a. if the system maps the user query to an incorrect event; 
- b. given a correct event mapping, if the system provides a correct response.

### quality
- evaluators rated the quality of the systemâ€™s prompt on a 7-point Likert scale, with the lowest quality being 1 and the highest quality being 7.
- aspects: length, tone, and details.

## Agent-Initiated Responses
- TODO